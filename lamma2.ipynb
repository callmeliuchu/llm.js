{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1567fa2-709f-47a2-be56-1d4e35db39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef9d53c-1571-4bd3-9c9b-18bb515a9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e061402-89d9-48e0-b217-ad6c40da2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfe60d8-f407-4972-8f64-9e819308657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1084ce-f953-4c97-bab7-28c936204ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any,Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9391ef08-6817-4adf-be25-37576a683b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2b975c-bac2-4689-bd32-b215de7d1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639526b6-576b-4376-ae5b-f22ef22637be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8db000-2950-4017-a714-a1349b7136f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b87c624-8e27-4730-bbd8-658005cde43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 32000\n",
    "    hidden_dim: Optional[int] = None\n",
    "    multiple_of: int = 256\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 2048\n",
    "    dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a1df86a-e36d-43a3-ad8c-f1e540bbe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,dim:int, eps:float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.dim))\n",
    "        \n",
    "    def _norm(self,x):\n",
    "        return x * torch.rsqrt(torch.square(x).mean(dim=-1,keepdim=True))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.weight * self._norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c286d567-7f1c-41d7-bc6a-9b3be274b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eec6c3d7-bb14-4d7f-8c48-846c40508407",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RMSNorm(2,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b82338b-bec8-44d2-93d5-469879ba3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rms(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a3c9d13-8326-4b0b-937d-8f450ebf7153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.square().mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "754d74b5-aaf1-45e5-809c-d28a27e36727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int,end: int,theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** torch.arange(0,dim,2)[:(dim // 2)].float()/dim)\n",
    "    t = torch.arange(end,device=freqs.device)\n",
    "    freqs = torch.outer(t,freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ea4defb-9c76-40cb-8c7f-566cd5a47156",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = precompute_freqs_cis(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c64e5c0a-38aa-423a-8629-bbaaa0ff2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor,x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1],x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim -1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb869768-1e90-4c64-9637-0b677d7a8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original freqs_cis shape: torch.Size([4, 6])\n",
      "Reshaped freqs_cis shape: torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 x 是一个形状为 [batch_size, seq_len, dim] 的张量\n",
    "x = torch.randn(2, 4, 6)  # [batch_size, seq_len, dim]\n",
    "\n",
    "# freqs_cis 是与 x 的第二个维度和最后一个维度相匹配的张量\n",
    "freqs_cis = torch.randn(4, 6)  # shape (seq_len, dim)\n",
    "\n",
    "# 调用 reshape_for_broadcast 函数\n",
    "reshaped_freqs_cis = reshape_for_broadcast(freqs_cis, x)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Original freqs_cis shape:\", freqs_cis.shape)\n",
    "print(\"Reshaped freqs_cis shape:\", reshaped_freqs_cis.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "614771be-864e-4ce3-9022-d035912dcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor,torch.Tensor]:\n",
    "\n",
    "    xq_r,xq_i = xq.float().reshape(xq.shape[:-1] + (-1,2)).unbind(-1)\n",
    "    xk_r,xk_i = xk.float().reshape(xk.shape[:-1] + (-1,2)).unbind(-1)\n",
    "\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos,xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin,xq_r)\n",
    "\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    xq_out = torch.stack([xq_out_r,xq_out_i],dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r,xk_out_i],dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e554d0ad-7c05-45a9-a7e3-f38127b79c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Query Tensor Shape:  torch.Size([2, 4, 4, 2])\n",
      "Output Key Tensor Shape:  torch.Size([2, 4, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 模拟输入张量\n",
    "xq = torch.randn(2, 4, 8)  # [batch_size, seq_len, dim]\n",
    "xk = torch.randn(2, 4, 8)\n",
    "\n",
    "# 模拟旋转频率（通常基于正弦和余弦函数生成）\n",
    "freqs_cos = torch.randn(4, 4)  # shape (seq_len, dim // 2)\n",
    "freqs_sin = torch.randn(4, 4)\n",
    "\n",
    "# 调用 apply_rotary_emb\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "print(\"Output Query Tensor Shape: \", xq_out.shape)\n",
    "print(\"Output Key Tensor Shape: \", xk_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "698fed74-ded4-4e11-8a4a-7955027873d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor,n_rep: int) -> torch.Tensor:\n",
    "    bs,slen,n_kv_heads,head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,:,None,:].expand(\n",
    "            bs,slen,n_kv_heads,n_rep,head_dim\n",
    "        ).reshape(\n",
    "            bs,slen,n_kv_heads * n_rep, head_dim\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7432946-b214-4106-9f0a-acc39e98f7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3, 4, 5])\n",
      "Result shape after repeating: torch.Size([2, 3, 12, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义输入张量 x，形状为 [batch_size, seq_len, n_kv_heads, head_dim]\n",
    "x = torch.randn(2, 3, 4, 5)  # 形状为 [2, 3, 4, 5]，batch_size=2, seq_len=3, n_kv_heads=4, head_dim=5\n",
    "\n",
    "# 需要重复的次数 n_rep\n",
    "n_rep = 3\n",
    "\n",
    "# 调用 repeat_kv 函数\n",
    "result = repeat_kv(x, n_rep)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Original shape:\", x.shape)\n",
    "print(\"Result shape after repeating:\", result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f4b87ba-e2d1-4749-86b1-e4a56d979a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,args: ModelArgs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        model_parallel_size = 1\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.wq = nn.Linear(args.dim,args.n_heads * self.head_dim,bias=False)\n",
    "        self.wk = nn.Linear(args.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(args.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim,bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_drop = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional,'scaled_dot_product_attention')\n",
    "\n",
    "        if not self.flash:\n",
    "            print(\"Warning\")\n",
    "            mask = torch.full((1,1,args.max_seq_len,args.max_seq_len),float(\"-inf\"))\n",
    "            mask = torch.triu(mask,diagonal=1)\n",
    "            self.register_buffer(\"mask\",mask)\n",
    "            \n",
    "    def forward(self,x:torch.Tensor,freqs_cos:torch.Tensor,freqs_sin:torch.Tensor):\n",
    "        bsz,seq_len,_ = x.shape\n",
    "        xq,xk,xv = self.wq(x),self.wk(x),self.wv(x)\n",
    "        xq = xq.view(bsz,seq_len,self.n_local_heads,self.head_dim)\n",
    "        xk = xk.view(bsz,seq_len,self.n_local_kv_heads,self.head_dim)\n",
    "        xv = xv.view(bsz,seq_len,self.n_local_kv_heads,self.head_dim)\n",
    "        xq,xk = apply_rotary_emb(xq,xk,freqs_cos,freqs_sin)\n",
    "\n",
    "        xk = repeat_kv(xk,self.n_rep)\n",
    "        xv = repeat_kv(xv,self.n_rep)\n",
    "\n",
    "\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = xk.transpose(1,2)\n",
    "        xv = xv.transpose(1,2)\n",
    "\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq,xk,xv,attn_mask=None,\n",
    "                                                                     dropout_p=self.dropout if self.training else 0.0,is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq,xk.transpose(2,3))\n",
    "            assert hasattr(self,'mask')\n",
    "            scores = scores + self.mask[:,:,:seq_len,:seq_len]\n",
    "            scores = F.softmax(scores.float(),dim=-1).tyype_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores,xv)\n",
    "        output = output.transpose(1,2).contiguous().view(bsz,seq_len,-1)\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_drop(output)\n",
    "        return output\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4298181-60fe-4ff8-99fc-0a756a2cd4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m x, freqs_cos, freqs_sin \u001b[38;5;241m=\u001b[39m generate_random_tensors(args)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 输出结果\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\software\\miniconda\\envs\\minitorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\software\\miniconda\\envs\\minitorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 35\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m     33\u001b[0m xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mview(bsz,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_kv_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m     34\u001b[0m xv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mview(bsz,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_kv_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m---> 35\u001b[0m xq,xk \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m xk \u001b[38;5;241m=\u001b[39m repeat_kv(xk,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rep)\n\u001b[0;32m     38\u001b[0m xv \u001b[38;5;241m=\u001b[39m repeat_kv(xv,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rep)\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[1;34m(xq, xk, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m      8\u001b[0m xq_r,xq_i \u001b[38;5;241m=\u001b[39m xq\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(xq\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m xk_r,xk_i \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(xk\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m freqs_cos \u001b[38;5;241m=\u001b[39m \u001b[43mreshape_for_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxq_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m freqs_sin \u001b[38;5;241m=\u001b[39m reshape_for_broadcast(freqs_sin,xq_r)\n\u001b[0;32m     14\u001b[0m xq_out_r \u001b[38;5;241m=\u001b[39m xq_r \u001b[38;5;241m*\u001b[39m freqs_cos \u001b[38;5;241m-\u001b[39m xq_i \u001b[38;5;241m*\u001b[39m freqs_sin\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mreshape_for_broadcast\u001b[1;34m(freqs_cis, x)\u001b[0m\n\u001b[0;32m      2\u001b[0m ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ndim\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m shape \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m ndim \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mview(shape)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# 随机生成频率张量和输入张量\n",
    "def generate_random_tensors(args):\n",
    "    x = torch.randn(2, args.max_seq_len, args.dim)  # 输入 (bsz, seq_len, dim)\n",
    "    freqs_cos = torch.randn(args.max_seq_len, args.dim // args.n_heads)\n",
    "    freqs_sin = torch.randn(args.max_seq_len, args.dim // args.n_heads)\n",
    "    return x, freqs_cos, freqs_sin\n",
    "\n",
    "# 使用模型参数和Attention类\n",
    "args = ModelArgs(dim=512, n_heads=8)\n",
    "attention = Attention(args)\n",
    "\n",
    "# 随机输入\n",
    "x, freqs_cos, freqs_sin = generate_random_tensors(args)\n",
    "\n",
    "# 前向传播\n",
    "output = attention(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4c0adb5-628a-408f-a344-6998f9047e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self,dim:int,hidden_dim:int,multiple_of:int,dropout:float):\n",
    "\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim,dim,bias=False)\n",
    "        self.w3 = nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x))*self.w3(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3943d9b-de2a-4a9f-94af-cadb1b480e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义FeedForward类（如上所述）\n",
    "\n",
    "# 模拟输入张量 x\n",
    "x = torch.randn(10, 512)  # 假设输入大小为 (batch_size=10, dim=512)\n",
    "\n",
    "# 定义FeedForward层\n",
    "ffn = FeedForward(dim=512, hidden_dim=None, multiple_of=64, dropout=0.1)\n",
    "\n",
    "# 前向传播\n",
    "output = ffn(x)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d17953d0-9888-4f9f-a112-70d5df25f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,layer_id: int,args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim = args.dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            dropout=args.dropout\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim,eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim,eps=args.norm_eps)\n",
    "    def forward(self,x,freqs_cos,freqs_sin):\n",
    "        h = x + self.attention.forward(self.attention_norm(x),freqs_cos,freqs_sin)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51cdfb90-5542-4dd3-b5ec-cc7b2a760f7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m freqs_sin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 输出结果\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\software\\miniconda\\envs\\minitorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\software\\miniconda\\envs\\minitorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 19\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,freqs_cos,freqs_sin):\n\u001b[1;32m---> 19\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[58], line 35\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m     33\u001b[0m xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mview(bsz,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_kv_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m     34\u001b[0m xv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mview(bsz,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_kv_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m---> 35\u001b[0m xq,xk \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m xk \u001b[38;5;241m=\u001b[39m repeat_kv(xk,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rep)\n\u001b[0;32m     38\u001b[0m xv \u001b[38;5;241m=\u001b[39m repeat_kv(xv,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rep)\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[1;34m(xq, xk, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m      8\u001b[0m xq_r,xq_i \u001b[38;5;241m=\u001b[39m xq\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(xq\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m xk_r,xk_i \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(xk\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m freqs_cos \u001b[38;5;241m=\u001b[39m \u001b[43mreshape_for_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxq_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m freqs_sin \u001b[38;5;241m=\u001b[39m reshape_for_broadcast(freqs_sin,xq_r)\n\u001b[0;32m     14\u001b[0m xq_out_r \u001b[38;5;241m=\u001b[39m xq_r \u001b[38;5;241m*\u001b[39m freqs_cos \u001b[38;5;241m-\u001b[39m xq_i \u001b[38;5;241m*\u001b[39m freqs_sin\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mreshape_for_broadcast\u001b[1;34m(freqs_cis, x)\u001b[0m\n\u001b[0;32m      2\u001b[0m ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ndim\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m shape \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m ndim \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mview(shape)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 随机输入\n",
    "args = ModelArgs(dim=512, n_heads=8, hidden_dim=1024, multiple_of=64, dropout=0.1, norm_eps=1e-5)\n",
    "block = TransformerBlock(layer_id=1, args=args)\n",
    "\n",
    "# 创建随机输入\n",
    "x = torch.randn(10, 128, 512)  # 假设 (batch_size=10, seq_len=128, dim=512)\n",
    "freqs_cos = torch.randn(128, 512 // 8)  # 频率张量\n",
    "freqs_sin = torch.randn(128, 512 // 8)\n",
    "\n",
    "# 前向传播\n",
    "output = block(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ffa4e16-27d5-4f92-95c4-59b6d7627c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    last_loss: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embeddings.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # some useful precompute for the RoPE relative positional embeddings\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * params.n_layers))\n",
    "\n",
    "        # Initialize attribute for the loss of the last forward call. This will be set if the forward is called with a targets tensor.\n",
    "        self.last_loss = None\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        h = self.dropout(h)\n",
    "        freqs_cos = self.freqs_cos[:seqlen]\n",
    "        freqs_sin = self.freqs_sin[:seqlen]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(h[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            self.last_loss = None\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = sum(p.numel() for p in self.parameters())\n",
    "        cfg = self.params\n",
    "        L, H, Q, T = cfg.n_layers, cfg.n_heads, cfg.dim//cfg.n_heads, cfg.max_seq_len\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        Also note this is a super inefficient version of sampling with no key/value cache.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # crop to just the final time step\n",
    "            if temperature == 0.0:\n",
    "                # \"sample\" the single most likely index\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                # pluck the logits at the final step and scale by desired temperature\n",
    "                logits = logits / temperature\n",
    "                # optionally crop the logits to only the top k options\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                # apply softmax to convert logits to (normalized) probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9694af15-df47-4116-940e-28f78c8f88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5163,  0.8264,  0.1863,  ...,  1.0220, -2.0998,  0.3624],\n",
      "         [ 1.2761,  0.5518, -0.5195,  ...,  1.4396, -0.0480, -0.0997],\n",
      "         [-0.6359,  1.4571, -0.1298,  ...,  0.3464, -3.3644, -1.8127],\n",
      "         ...,\n",
      "         [ 0.2648,  2.2320,  0.3610,  ...,  0.7244, -2.0868, -2.7610],\n",
      "         [ 1.1659,  1.1208,  0.8284,  ..., -0.9833, -1.4072, -1.0673],\n",
      "         [ 1.4421,  1.6555,  2.3052,  ...,  1.1808, -2.7671, -1.1113]],\n",
      "\n",
      "        [[ 2.0097, -0.9510,  0.9372,  ..., -0.6056,  0.4065, -0.4302],\n",
      "         [ 1.0257,  0.4921,  0.0514,  ...,  0.6228, -0.0052,  1.3652],\n",
      "         [ 1.1492, -0.1643, -2.6517,  ...,  1.0021, -0.1882, -0.1027],\n",
      "         ...,\n",
      "         [-0.2462, -0.4856,  2.1299,  ..., -0.6836,  2.3092, -1.5769],\n",
      "         [ 1.9551,  0.8440,  1.7641,  ..., -0.3515,  0.8568, -0.0155],\n",
      "         [ 1.1114, -1.3413, -0.4130,  ..., -1.4152,  0.8416, -1.8739]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "num decayed parameter tensors: 225, with 6,607,077,376 parameters\n",
      "num non-decayed parameter tensors: 65, with 266,240 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# # 假设 ModelArgs 是一个包含模型参数的类\n",
    "# class ModelArgs:\n",
    "#     vocab_size = 1000  # 词汇表大小\n",
    "#     dim = 512          # 嵌入和隐藏层维度\n",
    "#     n_layers = 6       # Transformer 层数\n",
    "#     n_heads = 8        # 注意力头数\n",
    "#     dropout = 0.1      # Dropout 概率\n",
    "#     norm_eps = 1e-6    # 归一化的 epsilon\n",
    "#     max_seq_len = 100  # 最大序列长度\n",
    "\n",
    "params = ModelArgs()\n",
    "\n",
    "# 初始化 Transformer 模型\n",
    "model = Transformer(params)\n",
    "\n",
    "# 输入数据：tokens 和 targets\n",
    "tokens = torch.randint(0, params.vocab_size, (2, 10))  # 假设有2个序列，每个序列10个 token\n",
    "targets = torch.randint(0, params.vocab_size, (2, 10))\n",
    "\n",
    "# 前向传播\n",
    "logits = model(tokens, targets)\n",
    "\n",
    "# 输出 logits\n",
    "print(logits)\n",
    "\n",
    "# 配置优化器\n",
    "optimizer = model.configure_optimizers(weight_decay=0.01, learning_rate=1e-3, betas=(0.9, 0.999), device_type='cuda')\n",
    "\n",
    "# 生成新 token\n",
    "generated_tokens = model.generate(tokens, max_new_tokens=5, temperature=1.0, top_k=50)\n",
    "\n",
    "print(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ede50b5-06dc-4dba-b515-61bd5a201250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],\n",
      "        [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],\n",
      "        [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "        [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03]])\n",
      "theta tensor([1.0000, 0.1000, 0.0100, 0.0010])\n",
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],\n",
      "        [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],\n",
      "        [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],\n",
      "        [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def precompute_theta_pos_frequencies(head_dim, seq_len, device, theta=10000.0):\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    print('freqs',freqs)\n",
    "    print('theta',theta)\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "# 参数设置\n",
    "head_dim = 8  # 头维度\n",
    "seq_len = 5   # 序列长度\n",
    "device = 'cpu'  # 设备选择\n",
    "\n",
    "# 调用函数\n",
    "frequencies = precompute_theta_pos_frequencies(head_dim, seq_len, device)\n",
    "\n",
    "# 打印结果\n",
    "print(frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a495eb-a555-4344-801e-894b03399299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207e41b-5730-4769-9e8c-cfc1532ce89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch Environment",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
