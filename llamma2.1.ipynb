{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ca2fd2-6361-4517-88fe-24be08c12f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e2b26e-8151-43f3-a568-a910a8cd9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e97f794-e844-4d02-93fc-dfa2c8679eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2f97d0-b001-4177-838b-8e54fa7bc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0d8ca8-0009-43dd-8e2a-927e7b9b8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any,Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0f8188-59f6-4018-8592-d3d5a8d26d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a2f0cb8-a970-42e5-9599-3e53419a9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f31c6fd4-5b5f-4b65-9d60-1aecc64db23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d064fae5-6bc8-415e-bbc3-e35fc28ef1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0dbf256-899a-46c2-b328-ac7fd0ecd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the Llama 7B model\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 32000\n",
    "    hidden_dim: Optional[int] = None\n",
    "    multiple_of: int = 256  # MLP hidden layer size will be multiple of\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 2048\n",
    "    dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46105241-540b-44d7-aac7-b4ab06ab0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,eps=0.0):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # root = (x ** 2).mean(dim=-1,keepdim=True) ** 0.5\n",
    "        root = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return self.weights * (x * (root + self.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4447e84b-b3e5-48d1-8d3d-e1bddd7e53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RMSNorm(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1e18d3-684a-487a-90bc-25784aa76b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33549019-aeb3-4993-b756-8fa67c540fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=rms(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5850a4c-8ca9-4956-8ef4-b228c6ba5554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7215, 0.5718, 0.2738, 0.7965],\n",
       "        [1.0627, 1.0709, 0.9630, 0.8925],\n",
       "        [0.6462, 1.3204, 1.3475, 0.1526],\n",
       "        [0.6397, 0.9529, 1.3306, 0.9552],\n",
       "        [0.6170, 1.1709, 1.0955, 1.0238]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66de54c-b287-46e9-a137-63b941583b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y ** 2).mean(dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a2b0d2-0d8a-4f35-8122-48495a7b38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim:int,end:int,theta:float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0,dim,2)[:(dim//2)].float() / dim) )\n",
    "    t = torch.arange(end)\n",
    "    freqs = torch.outer(t,freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos,freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3110d541-b5a9-425d-8486-a44fd822c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = precompute_freqs_cis(4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1732244-2a57-47b6-babd-bc6b01d47537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis:torch.Tensor,x:torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1],x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d3686a-b1a3-49e0-969d-fc802b3a2ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000],\n",
       "        [ 0.5403,  0.9999],\n",
       "        [-0.4161,  0.9998],\n",
       "        [-0.9900,  0.9996],\n",
       "        [-0.6536,  0.9992],\n",
       "        [ 0.2837,  0.9988],\n",
       "        [ 0.9602,  0.9982],\n",
       "        [ 0.7539,  0.9976]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "329000b3-2e5b-48b8-9bc3-f5687e60a61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_for_broadcast(x1,torch.ones([1,8,3,2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1187010f-6c80-4aa5-a484-fcbbf2d2f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(xq:torch.Tensor,xk:torch.Tensor,freqs_cos:torch.Tensor,freqs_sin:torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:\n",
    "    xq_r,xq_i = xq.float().reshape(xq.shape[:-1]+(-1,2)).ubind(-1)\n",
    "    xk_r,xk_i = xk.float().reshape(xk.shape[:-1]+(-1,2)).ubind(-1)\n",
    "\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos,xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin,xq_r)\n",
    "\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "\n",
    "    xq_out = torch.stack([xq_out_r,xq_out_i],dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r,xq_out_i],dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d2c34d-40b5-408f-8567-347855d526fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    bs,slen,n_kv_heads,head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,:,None,:].expand(\n",
    "            bs,slen,n_kv_heads,n_rep,head_dim\n",
    "        ).reshape(bs,slen,n_kv_heads * n_rep,head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77a4500f-85b4-460a-90ea-f8b761ec0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e9866f7-d83d-4116-bf56-f0c67bb935b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d10e14-5a8b-4c98-ac67-d6f3bd87cbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_kv(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afbce864-eb9d-4c4f-be78-673eba44ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        model_parrallel_size = 1\n",
    "        self.n_local_heads = args.n_heads // model_parrallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parrallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.wq = nn.Linear(args.dim,self.n_heads * self.head_dim,bias=False)\n",
    "        self.wk = nn.Linear(args.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(args.dim,self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim,bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional,'scaled_dot_production_attention')\n",
    "        if not self.flash:\n",
    "            print('warning')\n",
    "            mask = torch.full((1,1,args.max_seq_len,args.max_seq_len),float('-inf'))\n",
    "            mask = torch.triu(mask,diagnoal=1)\n",
    "            self.register_buffer(\"mask\",mask)\n",
    "    def forward(self,x:torch.Tensor,freqs_cos:torch.Tensor,freqs_sin:torch.Tensor):\n",
    "        bsz, seqlen,_ = x.shape\n",
    "        xq,xk,xv = self.wq(x),self.wk(x),self.wv(x)\n",
    "        xq = xq.view(bsz,seqlen,self.n_local_heads,self.head_dim)\n",
    "        xk = xk.view(bsz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "        xv = xv.view(bsz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "        xq,xv = apply_rotary_emb(xq,xv,freqs_cos,freqs_sin)\n",
    "        xk = repeat_kv(xk,self.n_rep)\n",
    "        xv = repeat_kv(xv,self.n_rep)\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = xk.transpose(1,2)\n",
    "        xv = xv.transpose(1,2)\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq,xk,xv,attn_mask=None,dropout=self.dropout if self.training\n",
    "                                                                     else 0.0,is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq,xk.transpose(2,3) / math.sqrt(self.head_dim))\n",
    "            assert hasattr(self,'mask')\n",
    "            scores = scores + self.mask[:,:,:seqlen,:seqlen]\n",
    "            scores = F.softmax(scores.float(),dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores,xv)\n",
    "        output = output.transpose(1,2).contiguous(),view(bsz,seqlen,-1)\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c4e0685-ac0e-4f8b-9ece-8b577af6f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self,dim: int,hidden_dim: int, multiple_of: int, dropout: float):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim,dim,bias=False)\n",
    "        self.w3 = nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a668618-ba25-412c-a153-7e53c65288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,layer_id:int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_head\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            dropout=args.dropout\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim,eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim,eps=args.norm_eps)\n",
    "    \n",
    "    def forward(self,x,freqs_cos,freqs_sin):\n",
    "        h = x + self.attention.forward(self.attention_norm(x),freqs_cos,freqs_sin)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "545ccded-edda-4b67-8366-e2e3474a3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    last_loss = Optional[torch.Tensor]\n",
    "    def __init__(self,params:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(params.vocab_size,params.dim)\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id,params))\n",
    "        self.norm = RMSNorm(params.dim,eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim,params.vocab_size,bias)\n",
    "        self.tok_embedding = self.output.weight\n",
    "        freqs_cos,freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * params.n_layers))\n",
    "\n",
    "        # Initialize attribute for the loss of the last forward call. This will be set if the forward is called with a targets tensor.\n",
    "        self.last_loss = None\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self,tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embedding(tokens)\n",
    "        h = self.dropout(h)\n",
    "        freqs_cos = self.freqs_cos[:seqlen]\n",
    "        freqs_sin = self.freqs_sin[:seqlen]\n",
    "        for layer in self.layers:\n",
    "            h = layer(h,freqs_cos,freqs_sin)\n",
    "        h = self.norm(h)\n",
    "        if targets is not None:\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.output(h[:,[-1],:])\n",
    "            self.last_loss = None\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630be7b-9e18-40d3-ae90-1a2fab99bde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch Environment",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
